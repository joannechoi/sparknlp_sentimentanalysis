{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from py4j.java_gateway import java_import\n",
    "java_import(spark._sc._jvm, \"org.apache.spark.sql.api.python.*\")\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"dateAdded\", StringType(), True),\n",
    "    StructField(\"dateUpdated\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"asins\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"primaryCategories\", StringType(), True),\n",
    "    StructField(\"imageURLs\", StringType(), True),\n",
    "    StructField(\"keys\", StringType(), True),\n",
    "    StructField(\"manufacturer\", StringType(), True),\n",
    "    StructField(\"manufacturerNumber\", StringType(), True),\n",
    "    StructField(\"reviews.date\", StringType(), True),\n",
    "    StructField(\"reviews.dateAdded\", StringType(), True),\n",
    "    StructField(\"reviews.dateSeen\", StringType(), True),\n",
    "    StructField(\"reviews.doRecommend\", StringType(), True),\n",
    "    StructField(\"reviews.id\", StringType(), True),\n",
    "    StructField(\"reviews.numHelpful\", StringType(), True),\n",
    "    StructField(\"reviews.rating\", IntegerType(), True),\n",
    "    StructField(\"reviews.sourceURLs\", StringType(), True),\n",
    "    StructField(\"reviews.text\", StringType(), True),\n",
    "    StructField(\"reviews.title\", StringType(), True),\n",
    "    StructField(\"reviews.username\", StringType(), True),\n",
    "    StructField(\"sourceURLs\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .schema(schema)\\\n",
    "    .csv(\"/Users/joanne/Documents/School/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\")\n",
    "\n",
    "for name in raw_data.schema.names:\n",
    "      raw_data = raw_data.withColumnRenamed(name, name.replace('.', '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, dateAdded: string, dateUpdated: string, name: string, asins: string, brand: string, categories: string, primaryCategories: string, imageURLs: string, keys: string, manufacturer: string, manufacturerNumber: string, reviews_date: string, reviews_dateAdded: string, reviews_dateSeen: string, reviews_doRecommend: string, reviews_id: string, reviews_numHelpful: string, reviews_rating: int, reviews_sourceURLs: string, reviews_text: string, reviews_title: string, reviews_username: string, sourceURLs: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- dateAdded: string (nullable = true)\n",
      " |-- dateUpdated: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- asins: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- primaryCategories: string (nullable = true)\n",
      " |-- imageURLs: string (nullable = true)\n",
      " |-- keys: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- manufacturerNumber: string (nullable = true)\n",
      " |-- reviews_date: string (nullable = true)\n",
      " |-- reviews_dateAdded: string (nullable = true)\n",
      " |-- reviews_dateSeen: string (nullable = true)\n",
      " |-- reviews_doRecommend: string (nullable = true)\n",
      " |-- reviews_id: string (nullable = true)\n",
      " |-- reviews_numHelpful: string (nullable = true)\n",
      " |-- reviews_rating: integer (nullable = true)\n",
      " |-- reviews_sourceURLs: string (nullable = true)\n",
      " |-- reviews_text: string (nullable = true)\n",
      " |-- reviews_title: string (nullable = true)\n",
      " |-- reviews_username: string (nullable = true)\n",
      " |-- sourceURLs: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------+------+--------------------+-----------------+--------------------+--------------------+------------+------------------+--------------------+-----------------+--------------------+-------------------+----------+------------------+--------------+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "|                  id|           dateAdded|         dateUpdated|                name|     asins| brand|          categories|primaryCategories|           imageURLs|                keys|manufacturer|manufacturerNumber|        reviews_date|reviews_dateAdded|    reviews_dateSeen|reviews_doRecommend|reviews_id|reviews_numHelpful|reviews_rating|  reviews_sourceURLs|        reviews_text|       reviews_title|reviews_username|          sourceURLs|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+------+--------------------+-----------------+--------------------+--------------------+------------+------------------+--------------------+-----------------+--------------------+-------------------+----------+------------------+--------------+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "|AVqVGZNvQMlgsOJE6eUY|2017-03-03T16:56:05Z|2018-10-25T16:36:31Z|\"Amazon Kindle E-...|B00ZV9PXP2|Amazon|ComputersElectron...|      Electronics|https://pisces.bb...|allnewkindleeread...|      Amazon|        B00ZV9PXP2|2017-09-03T00:00:...|             null|2018-05-27T00:00:...|              FALSE|      null|                 0|             3|http://reviews.be...|I thought it woul...|           Too small|          llyyue|https://www.neweg...|\n",
      "|AVqVGZNvQMlgsOJE6eUY|2017-03-03T16:56:05Z|2018-10-25T16:36:31Z|\"Amazon Kindle E-...|B00ZV9PXP2|Amazon|ComputersElectron...|      Electronics|https://pisces.bb...|allnewkindleeread...|      Amazon|        B00ZV9PXP2|2017-06-06T00:00:...|             null|2018-05-27T00:00:...|               TRUE|      null|                 0|             5|http://reviews.be...|This kindle is li...|Great light reade...|          Charmi|https://www.neweg...|\n",
      "|AVqVGZNvQMlgsOJE6eUY|2017-03-03T16:56:05Z|2018-10-25T16:36:31Z|\"Amazon Kindle E-...|B00ZV9PXP2|Amazon|ComputersElectron...|      Electronics|https://pisces.bb...|allnewkindleeread...|      Amazon|        B00ZV9PXP2|2018-04-20T00:00:...|             null|2018-05-27T00:00:00Z|               TRUE|      null|                 0|             4|https://reviews.b...|Didnt know how mu...| Great for the price|    johnnyjojojo|https://www.neweg...|\n",
      "|AVqVGZNvQMlgsOJE6eUY|2017-03-03T16:56:05Z|2018-10-25T16:36:31Z|\"Amazon Kindle E-...|B00ZV9PXP2|Amazon|ComputersElectron...|      Electronics|https://pisces.bb...|allnewkindleeread...|      Amazon|        B00ZV9PXP2|2017-11-02T17:33:...|             null|2018-10-09T00:00:00Z|               TRUE| 177283626|                 3|             5|https://redsky.ta...|I am 100 happy wi...|         A Great Buy|         Kdperry|https://www.neweg...|\n",
      "|AVqVGZNvQMlgsOJE6eUY|2017-03-03T16:56:05Z|2018-10-25T16:36:31Z|\"Amazon Kindle E-...|B00ZV9PXP2|Amazon|ComputersElectron...|      Electronics|https://pisces.bb...|allnewkindleeread...|      Amazon|        B00ZV9PXP2|2018-04-24T00:00:...|             null|2018-05-27T00:00:00Z|               TRUE|      null|                 0|             5|https://reviews.b...|Solid entry level...|Solid entry-level...|     Johnnyblack|https://www.neweg...|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+------+--------------------+-----------------+--------------------+--------------------+------------+------------------+--------------------+-----------------+--------------------+-------------------+----------+------------------+--------------+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                name|count|\n",
      "+--------------------+-----+\n",
      "|\"Amazon Echo Show...|  845|\n",
      "|\"All-New Fire HD ...|  797|\n",
      "|Amazon - Echo Plu...|  590|\n",
      "|Fire Kids Edition...|  561|\n",
      "|\"Brand New Amazon...|  467|\n",
      "|Fire Tablet 7 Dis...|  371|\n",
      "|Amazon Tap - Alex...|  225|\n",
      "|Fire Kids Edition...|  217|\n",
      "|Kindle E-reader -...|  159|\n",
      "|Fire HD 10 Tablet...|  106|\n",
      "|\"Fire Tablet with...|  101|\n",
      "|\"Amazon Kindle E-...|   96|\n",
      "|\"Amazon - Kindle ...|   82|\n",
      "|All-New Fire HD 8...|   70|\n",
      "|\"All-New Fire HD ...|   58|\n",
      "|\"Fire HD 8 Tablet...|   53|\n",
      "|All-New Fire HD 8...|   51|\n",
      "|\"All-New Fire HD ...|   40|\n",
      "|\"Kindle Oasis E-r...|   39|\n",
      "|Kindle Oasis E-re...|   24|\n",
      "|Amazon 9W PowerFa...|   22|\n",
      "|Amazon - Kindle V...|   22|\n",
      "|Amazon Fire TV wi...|    4|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.select('name').groupby('name').count().orderBy('count', ascending=False).show(raw_data.count(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|reviews_rating|count|\n",
      "+--------------+-----+\n",
      "|             1|   63|\n",
      "|             2|   54|\n",
      "|             3|  197|\n",
      "|             4| 1208|\n",
      "|             5| 3478|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.select('reviews_rating').groupBy('reviews_rating').count().orderBy('reviews_rating', ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|    reviews_rating|\n",
      "+-------+------------------+\n",
      "|  count|              5000|\n",
      "|   mean|            4.5968|\n",
      "| stddev|0.7318038448747551|\n",
      "|    min|                 1|\n",
      "|    max|                 5|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.select('reviews_rating').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------+\n",
      "|index|                name|        reviews_text|reviews_rating|\n",
      "+-----+--------------------+--------------------+--------------+\n",
      "|    0|\"Amazon Kindle E-...|I thought it woul...|             3|\n",
      "|    1|\"Amazon Kindle E-...|This kindle is li...|             5|\n",
      "|    2|\"Amazon Kindle E-...|Didnt know how mu...|             4|\n",
      "|    3|\"Amazon Kindle E-...|I am 100 happy wi...|             5|\n",
      "|    4|\"Amazon Kindle E-...|Solid entry level...|             5|\n",
      "|    5|\"Amazon Kindle E-...|This make an exce...|             5|\n",
      "|    6|\"Amazon Kindle E-...|I ordered this fo...|             5|\n",
      "|    7|\"Amazon Kindle E-...|I bought my Kindl...|             4|\n",
      "|    8|\"Amazon Kindle E-...|amazon kindle is ...|             5|\n",
      "|    9|\"Amazon Kindle E-...|It's beyond my ex...|             5|\n",
      "|   10|\"Amazon Kindle E-...|If you really wan...|             5|\n",
      "|   11|\"Amazon Kindle E-...|Love my kindle ma...|             5|\n",
      "|   12|\"Amazon Kindle E-...|Good product for ...|             5|\n",
      "|   13|\"Amazon Kindle E-...|This Kindle is a ...|             5|\n",
      "|   14|\"Amazon Kindle E-...|I use this every ...|             5|\n",
      "|   15|\"Amazon Kindle E-...|It does its job b...|             3|\n",
      "|   16|\"Amazon Kindle E-...|Great product for...|             5|\n",
      "|   17|\"Amazon Kindle E-...|Item work well an...|             4|\n",
      "|   18|\"Amazon Kindle E-...|Great product and...|             5|\n",
      "|   19|\"Amazon Kindle E-...|Everything is gre...|             4|\n",
      "+-----+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = raw_data.select('name','reviews_text','reviews_rating')\n",
    "reviews_df = spark.createDataFrame(reviews.toPandas().reset_index())\n",
    "reviews_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[reviews_text: string]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = raw_data.select('reviews_text')\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: bigint, name: string, reviews_text: string, reviews_rating: bigint]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = DocumentAssembler() \\\n",
    "            .setInputCol(\"reviews_text\") \\\n",
    "            .setOutputCol(\"document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[reviews_text: string, document: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = SentenceDetector() \\\n",
    "            .setInputCols([\"document\"]) \\\n",
    "            .setOutputCol(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceDetector_7b36498736e7"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.setExplodeSentences(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols([\"sentence\"]) \\\n",
    "            .setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellcheck_norvig download started this may take some time.\n",
      "Approximate size to download 4.2 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "checker = NorvigSweetingModel.pretrained() \\\n",
    "            .setInputCols([\"token\"]) \\\n",
    "            .setOutputCol(\"checked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_dl download started this may take some time.\n",
      "Approximate size to download 13.6 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "ner = NerDLModel.pretrained() \\\n",
    "        .setInputCols(['sentence','checked']) \\\n",
    "        .setOutputCol('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = NerConverter() \\\n",
    "            .setInputCols(['sentence', 'checked', 'ner']) \\\n",
    "            .setOutputCol('chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        document, \n",
    "        sentence, \n",
    "        tokenizer, \n",
    "        checker, \n",
    "        ner, \n",
    "        converter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o220.getParam.\n: java.util.NoSuchElementException: Param detectLists does not exist.\n\tat org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n\tat org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.ml.param.Params$class.getParam(params.scala:728)\n\tat org.apache.spark.ml.PipelineStage.getParam(Pipeline.scala:42)\n\tat sun.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-be7a4aea8339>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Dev/Apache-Spark/spark-2.4.8-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/Documents/Dev/Apache-Spark/spark-2.4.8-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Dev/Apache-Spark/spark-2.4.8-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Dev/Apache-Spark/spark-2.4.8-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Dev/Apache-Spark/spark-2.4.8-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transfer_params_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mpair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_java_param_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_defaultParamMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0mpair_defaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_defaults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Dev/Apache-Spark/spark-2.4.8-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_make_java_param_pair\u001b[0;34m(self, param, value)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolveParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mjava_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mjava_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Dev/Apache-Spark/spark-2.4.8-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Dev/Apache-Spark/spark-2.4.8-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Dev/Apache-Spark/spark-2.4.8-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o220.getParam.\n: java.util.NoSuchElementException: Param detectLists does not exist.\n\tat org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n\tat org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.ml.param.Params$class.getParam(params.scala:728)\n\tat org.apache.spark.ml.PipelineStage.getParam(Pipeline.scala:42)\n\tat sun.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
